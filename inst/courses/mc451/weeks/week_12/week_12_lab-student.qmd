---
title: "Week 12: Data Wrangling with Pew Research Data"
subtitle: "From Raw Information to Meaningful Insight"
format: html
editor: visual
---

> **Required Citation:**
> Pew Research Center, "ATP W144: Politics and News on Social Media Platforms."
> Pew Research Center, Washington, D.C. (March 2024).
>
> **Required Disclaimer:**
> Pew Research Center bears no responsibility for the analyses or interpretations of the data presented here. The opinions expressed herein, including any implications for policy, are those of the author and not of Pew Research Center.

## Introduction

This lab follows **Chapter 12: Data Wrangling**. Our goal is to take a "messy," partially-cleaned dataset and finish the wrangling process ourselves. We will focus on the "Cleaning" and "Transforming" phases to prepare our data for analysis.

Our primary task is to take a set of raw survey items about news sources on Facebook and create a single "Facebook News Trust" composite score.

## 1. Setup: Load Packages

First, we load the `tidyverse` (for all our data manipulation) and `mccoursepack` (which contains our course data).

```{r load-packages, message=FALSE, warning=FALSE}
library(tidyverse)
library(mccoursepack) # Assumes your package is named this
```

## 2. Phase 1: Importing & Inspecting

Instead of importing a raw file, we will load the `w144_excerpt` dataset from our course package. This data has 1,000 respondents and a pre-selected set of variables.

```{r load-data}
# Load the excerpted Pew data from our course package
data("w144_excerpt")
# Let's "interview" the data, as Chapter 12 suggests
glimpse(w144_excerpt)
```

**Take a look at the data.** You'll see clean factors like `gender_factor` and `party_factor`. But you'll also see "raw" variables like `fb_news_friends`, `fb_news_journalists`, etc. These are the variables we need to wrangle.

## 3. Phase 2: Cleaning the Scale

The `fb_news_...` variables measure how often people get news from different sources. The scale is:
This is a **reversed scale**: a *lower* number means *more* frequency. For analysis, it's more intuitive to have higher numbers mean more frequency. We need to **reverse code** these variables:
* `4` = Often (originally 1)
* `3` = Sometimes (originally 2)
* `2` = Hardly ever (originally 3)
* `1` = Never (originally 4)
Our desired new scale will be:
* `4` = Often (originally 1)
* `3` = Sometimes (originally 2)
* `2` = Hardly ever (originally 3)
* `1` = Never (originally 4)

```{r clean-data}
## Wrangle and recode the fb_news_ variables
w144_clean <- w144_excerpt %>%
  mutate(
    across(
      .cols = starts_with("fb_news_"),
      .fns = ~ recode(as.numeric(haven::zap_labels(.)), `1` = 4, `2` = 3, `3` = 2, `4` = 1),
      .names = "{.col}_r"
    ),
    # Create the composite score by averaging the five reverse-coded items
    fb_news_diversity_score = rowMeans(select(., ends_with("_r")), na.rm = TRUE),
    # Replace NaN with NA for cases where all five are missing
    fb_news_diversity_score = ifelse(is.nan(fb_news_diversity_score), NA, fb_news_diversity_score)
  )

# Let's check our work
w144_clean %>%
  select(fb_news_friends, fb_news_friends_r, fb_news_diversity_score) %>%
  head()
```

# Let's check our work
```{r check-data}
w144_clean %>%
  select(fb_news_friends, fb_news_friends_r, fb_news_diversity_score) %>%
  head()
```
## Now that our variables are cleaned and reverse-coded, our composite variable `fb_news_diversity_score` represents the average frequency of seeing news from diverse sources on Facebook. If all five items are missing for a respondent, their score will be NA.

## 4. Phase 3: Transforming Data

Now that our variables are cleaned and reverse-coded, we can create our new composite variable.

### A. Create a New Variable (Composite Score)

Let's create a "Facebook News Diversity" score by averaging the five reverse-coded items. This new variable will represent a person's average frequency of seeing news from diverse sources on Facebook.

```{r transform-composite}
w144_final <- w144_clean %>%
  mutate(
    # Create a new column 'fb_news_diversity_score'
    fb_news_diversity_score = rowMeans(
      select(., starts_with("fb_news_") & ends_with("_r")),
      na.rm = TRUE # This is VITAL. It ignores missing values.
    )
  )

# Look at our new composite score!
w144_final %>%
  select(fb_news_diversity_score, party_factor) %>%
  head()
```

### B. Aggregate and Summarize Data

Now we can use our new score to transform individual data into group-level summaries. What is the average "Facebook News Diversity" score for each political party?

```{r transform-aggregate}
party_summary <- w144_final %>%
  group_by(party_factor) %>%
  # Make sure to drop NA values from the party factor
  filter(!is.na(party_factor)) %>%
  summarize(
    # Calculate the mean, removing NAs
    avg_news_diversity = mean(fb_news_diversity_score, na.rm = TRUE),
    # Calculate the count (n())
    num_respondents = n()
  ) %>%
  mutate(
    avg_news_diversity = ifelse(is.nan(avg_news_diversity), NA, avg_news_diversity)
  )

# Print the summary table
print(party_summary)
```

## 5. Save Your Work

We've successfully wrangled the data! Let's save our final, clean data frame. We will use this file in Lab 13 and 14.

```{r save-data}
write_csv(w144_final, "w144_clean.csv")
print("Successfully created w144_clean.csv")
```
