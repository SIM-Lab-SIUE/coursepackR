---
title: "Week 12: Data Wrangling with Pew Research Data"
subtitle: "From Raw Information to Meaningful Insight"
format: html
editor: visual
---

## **COMM 451: Introduction to Data Analysis**

**Lab 12: Data Wrangling** **Due Date:** Week 12, end of lab period

### **Learning Objectives**

Welcome to your first *real* lab in RStudio! The process of cleaning and preparing data is called **data wrangling**. As the *data-chapters.pdf* reading explains, this is the "mise en place" of research‚Äîthe essential prep work you do before you can get to the "cooking" (or analysis).

By the end of this lab, you will be able to:

-   Navigate the RStudio interface.
-   Read a dataset into R using `read_csv()`.
-   Create new variables (columns) using `mutate()`.
-   Use `case_when()` and `if_else()` to recode data.
-   Combine multiple variables to create a **composite scale score**.
-   Answer basic descriptive questions using `group_by()` and `summarize()`.

### **Pew Research Center: Required Citation**

This lab uses a real-world dataset from the Pew Research Center. As per their Terms of Use, you must always include the following disclaimer in any work (even class assignments) that uses this data:

> "Pew Research Center bears no responsibility for the analyses or interpretations of the data presented here. The opinions expressed herein, including any implications for policy, are those of the author and not of Pew Research Center.‚Äù

------------------------------------------------------------------------

## **Part 1: Orientation - Welcome to RStudio**

First, open RStudio. You will see four main windows, or **panes**.

1.  **Top-Left (Script Editor):** This is your **R Script** (a `.R` file). It's like a text document where you write and save all your code. This is where you will do 99% of your work.
2.  **Bottom-Left (Console):** This is the **Console**. It's where your code actually *runs* and where results appear. You can type code here, but it's not saved.
3.  **Top-Right (Environment):** This is your **Environment**. It's a list of all the **objects** (like datasets) that you have loaded into R's memory.
4.  **Bottom-Right (Files/Plots):** This pane has multiple tabs. You'll use the **Files** tab to find your dataset and the **Plots** tab to see your visualizations.

### **The MOST Important First Step: RStudio Projects**

We will *always* use RStudio Projects. A "Project" bundles all your files (data, scripts, plots) into one folder. This saves you from *ever* having `file not found` errors.

1.  Go to **File \> New Project...**
2.  Click **New Directory**.
3.  Click **New Project**.
4.  For "Directory name," type something like **`R_Labs`**.
5.  Click **Create Project**.
6.  Now, go to the **Files** pane (bottom-right). Find the dataset file you were given, `w144_teaching_dataset_v2.csv`, and move it into this new `R_Labs` folder on your computer.
7.  Click the "New File" icon (white page with green plus) and select **R Script**. A new, blank `Script` will open (top-left).
8.  Go to **File \> Save As** and name this script `Lab_12.R`.

You are now ready. All your work will be saved in this project folder.

------------------------------------------------------------------------

## **Part 2: Setup - Loading Your Tools and Data**

Let's write some code! In your `Lab_12.R` script (top-left), type the following.

To **run** a line of code, click on it and press **`Ctrl + Enter`** (on Windows) or **`Cmd + Return`** (on Mac).

### **Step 1: Load the `tidyverse` üß∞**

R's power comes from packages. The `tidyverse` is a collection of packages that makes data wrangling simple and intuitive. The `library()` function "opens the toolbox" so R can use its functions.

```{r}
# This comment explains your code. R ignores anything after a #.
# Run this line to load all the 'tidyverse' tools.
library(tidyverse) 
```

*You may see some red text in your Console. This is normal! It's just R telling you which packages it loaded.*

### **Step 2: Load Your Data ‚û°Ô∏è**

Now we'll read our CSV file into the R **Environment**. We use the `read_csv()` function.

Most importantly, we need to *save* this data as an **object** (or "variable"). We do this using the **assignment arrow: `<-`** (a "less than" sign and a hyphen). You can think of `<-` as "gets" or "is saved as."

```{r}
# 1. Find the full path to the data file *inside* the 'mccoursepack' package
# system.file() is the standard way to find package files
data_path <- system.file("extdata", "w144_teaching_dataset_v2.csv", package = "mccoursepack")

# 2. Load the data from that full path
w144_data <- read_csv(data_path)
```

After you run this line, look at your **Environment** (top-right). You should see `w144_data` appear! This means your data is loaded.

### **Step 3: Look at Your Data ‚úÖ**

Never work "blind." Always look at your data after you load it. The best way is with the `glimpse()` function.

```{r}
# Run this to get a "glimpse" of your data in the Console.
glimpse(w144_data)
```

This is fantastic! `glimpse()` shows you every column name (like `party` or `uses_tiktok`), the data type (like `<fct>` for "factor" or `<dbl>` for "double"/number), and the first few rows of data.

------------------------------------------------------------------------

## **Part 3: The Core Task - Wrangling with `dplyr`**

We will now "wrangle" this data to create new variables for our analysis. We will use a "pipe," which looks like this: **`%>%`**.

The **pipe `%>%`** is the most important part of the `tidyverse`. It means **"AND THEN..."**

You use it to chain commands together in a logical, step-by-step way. `DATA %>% DO_STEP_1() %>% DO_STEP_2()`

Let's start a new "wrangled" dataset.

```{r}
# This code block starts our pipe.
# We're saying: "Take w144_data AND THEN..."
# We save the *entire* result in a new object: w144_wrangled
w144_wrangled <- w144_data %>% 
  
  # The 'mutate()' function CREATES NEW COLUMNS.
  # Let's start by making a simpler party variable.
  mutate(
    
    # 1. NEW VARIABLE: 'party_simple'
    # 'case_when()' is a powerful IF/THEN function.
    # It checks a set of rules in order.
    party_simple = case_when(
      party == "Republican" ~ "Republican", # IF party is "Republican", THEN "Republican"
      party == "Democrat" ~ "Democrat",     # IF party is "Democrat", THEN "Democrat"
      TRUE ~ "Independent/Other"          # TRUE is a "catch-all" for everything else
    ) %>% 
      # We pipe 'party_simple' to factor() to set the correct order
      factor(levels = c("Republican", "Democrat", "Independent/Other"))
  
  ) # This parenthesis closes the 'mutate()' function


# --- CHECK YOUR WORK ---
# Let's count our two party variables to see if it worked.
# The 'count()' function is a fast way to get frequencies.
w144_wrangled %>% 
  count(party, party_simple)
```

**Look at the result in your Console.** You should see a table showing how "Independent" and "Something else" were both correctly grouped into "Independent/Other."

------------------------------------------------------------------------

## **Part 4: Task 2 - Create a `platform_count` Scale**

Now, let's continue our pipe. We want to create a new variable that is a *count* of how many social media platforms a person uses (out of the four listed).

**How it works:** In R, when you do math on a logical statement, `TRUE` is treated as `1` and `FALSE` is treated as `0`. So, `(uses_facebook == "Yes")` will either be `TRUE` (1) or `FALSE` (0) for each person. This is a *very* powerful trick.

Let's add to our `mutate()` command.

```{r}
# Copy and paste this into your script.
# We are adding a comma after 'party_simple' and adding our new variable.

w144_wrangled <- w144_wrangled %>% 
  
  mutate(
    
    # 2. NEW VARIABLE: 'platform_count' (NEW CODE)
    # We are just adding the 1s and 0s from these four columns.
    platform_count = (uses_facebook == "Yes") +
                     (uses_x == "Yes") +
                     (uses_instagram == "Yes") +
                     (uses_tiktok == "Yes")
  
  ) # This parenthesis closes the 'mutate()' function


# Let's get a 'summary()' of our new numeric variable
summary(w144_wrangled$platform_count)
```

**In your Console,** you should see a summary showing the `Min.` (0), `Median`, `Mean`, and `Max.` (4) for your new scale. This proves it worked!

------------------------------------------------------------------------

## **Part 5: Task 3 - Create a "Reasons for Use" Scale**

This is the central task of the lab and a key skill for social science. We want to create a scale that measures *how many* different reasons a person uses Facebook (out of the 7 `fb_why_` items).

**The Problem:** The `fb_why_` columns are "Yes" and "No" text. We can't add text. **The Solution:** We need to do this in two steps:

1.  **Step 3a:** Convert all "Yes" to `1` and "No" to `0`.
2.  **Step 3b:** Add those `1`s and `0`s together for each person.

We will add these new steps *inside* our `mutate()` call.

```{r}
# Copy and paste this to *replace* your previous code.
# We are adding our new scale-creation steps.

w144_wrangled <- w144_wrangled %>% 
  
  mutate(

    # --- 3a. RECODE 'fb_why' ITEMS TO 1/0 ---
    # 'across()' lets us do the same thing to many columns at once.
    # 'starts_with("fb_why_")' selects all 7 'fb_why' columns.
    # 'if_else()' is a simple IF/THEN: if_else(condition, value_if_TRUE, value_if_FALSE)
    across(
      starts_with("fb_why_"), 
      ~ if_else(. == "Yes", 1, 0),
      # '.names' creates new columns, adding "_n" (for 'numeric') to the end
      .names = "{.col}_n" 
    )

  ) %>% # <-- This parenthesis closes 'mutate()'
  
  # --- 3b. SUM THE NEW 1/0 COLUMNS ---
  # We start a NEW 'mutate()' call after the first one finishes.
  # This is because we can only sum the columns *after* they are created.
  mutate(
    
    # 'rowSums()' adds up columns *across* a single row.
    # 'na.rm = TRUE' tells R to just ignore any missing (NA) data.
    fb_uses_count = rowSums(
      across(starts_with("fb_why_") & ends_with("_n")), # Selects our new numeric columns
      na.rm = TRUE
    )
    
  ) # <-- This parenthesis closes the SECOND 'mutate()'


# --- CHECK YOUR WORK ---
# Glimpse the data to see your new '_n' columns AND 'fb_uses_count'
glimpse(w144_wrangled)

# Get a summary of your final scale
summary(w144_wrangled$fb_uses_count)
```

**Check your `glimpse()` output.** You should see new columns like `fb_why_news_n` (which is `<dbl>`) and, at the very end, your new `fb_uses_count` scale! The summary should show a `Min.` of 0 and a `Max.` of 7.

------------------------------------------------------------------------

## **Part 6: The Payoff - Answering Questions**

Why did we do all that? Because now we can *finally* answer interesting questions. This is the "payoff" for all your hard wrangling.

**Question:** What is the average `platform_count` and `fb_uses_count` for each political party?

To do this, we combine two *new* verbs:

-   `group_by()`: Creates "groups" in the data (e.g., all Republicans, all Democrats).
-   `summarize()`: Collapses each group into a single number (like a mean).

<!-- end list -->

```{r}
# We use our final 'w144_wrangled' object
w144_wrangled %>% 
  
  # 1. Group by the 'party_simple' variable we created
  group_by(party_simple) %>% 
  
  # 2. Calculate the mean for our two new scales
  summarize(
    avg_platform_count = mean(platform_count, na.rm = TRUE),
    avg_fb_uses_count = mean(fb_uses_count, na.rm = TRUE)
  ) %>% 
  
  # 3. 'arrange()' sorts our final output
  # 'desc()' means "descending" (highest to lowest)
  arrange(desc(avg_platform_count))
```

**Look at the table in your Console.** You just ran your first analysis! You've transformed raw survey data into a clean, summary table that answers a specific research question.

------------------------------------------------------------------------

## **Part 7: Lab Submission (Optional Challenge)**

To get credit for this lab, save your `Lab_12.R` script.

If you have extra time, try this **challenge**:

1.  Go back into your `w144_wrangled` script.
2.  In the same way you created `fb_uses_count`, can you create a **`tt_uses_count`** scale using the `tt_why_` variables? (You'll need to add steps for `tt_why_..._n` and `tt_uses_count`).
3.  Run a final analysis that shows the average `tt_uses_count` for each `age_group`.

**Congratulations!** You've just completed the hardest part of data analysis: data wrangling. You are now officially an R user.