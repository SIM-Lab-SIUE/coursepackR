# **Lab 14: Inferential Statistics**

## **Learning Objectives**

In the last two labs, you have wrangled and visualized your data.

  * In **Lab 12**, you created new variables (like `platform_count`).
  * In **Lab 13**, you *described* those variables using plots (like `geom_violin`). You *saw* what looked like differences between groups.

This week, we make the **leap from description to inference**. As the `data-chapters.pdf` reading explains, we are moving from "what we see" in our **sample** to making an educated, statistical claim about the larger **population**. We will ask: "Is the difference I see in my plot *real*, or could it just be due to random chance?"

By the end of this lab, you will be able to:

  * Define a **Null Hypothesis (H0)** and an **Alternative Hypothesis (HA)**.
  * Understand and interpret a **p-value** (and the `p < .05` rule).
  * Run and interpret a **Chi-Square Test** (`chisq.test()`) for categorical relationships.
  * Run and interpret a **T-Test** (`t.test()`) to compare two group means.
  * Run and interpret an **ANOVA** (`aov()`) to compare three or more group means.
  * Run and interpret a **TukeyHSD()** post-hoc test to find *which* groups are different.

### **Pew Research Center: Required Citation**

As always, any script, report, or presentation that uses this data must include the following disclaimer:

> "Pew Research Center bears no responsibility for the analyses or interpretations of the data presented here. The opinions expressed herein, including any implications for policy, are those of the author and not of Pew Research Center.‚Äù

-----

## **Part 1: Setup - Get Your Data Ready**

This lab **depends on your work from Lab 12**. We must first re-create the `w144_wrangled` dataset that contains all your new scales and variables.

1.  Open RStudio and create a **new R Script**.
2.  Save it in your Project folder as `Lab_14.R`.
3.  Load the `tidyverse` library.

### **Step 1: The "Wrangle Chunk" üß±**

Copy this *entire* block of code from Lab 12. This is your "wrangle chunk" that creates your final `w144_wrangled` object.

**Run this entire chunk of code.**

```{r}
library(tidyverse)

data_path <- system.file("extdata", "w144_teaching_dataset_v2.csv", package = "mccoursepack")

w144_data <- read_csv(data_path)

glimpse(w144_data)

w144_wrangled <- w144_data %>% 
  
  mutate(
    
    party_simple = case_when(
      party == "Republican" ~ "Republican",
      party == "Democrat" ~ "Democrat",
      TRUE ~ "Independent/Other"
    ) %>% 
      factor(levels = c("Republican", "Democrat", "Independent/Other")),
    
    platform_count = (uses_facebook == "Yes") +
                     (uses_x == "Yes") +
                     (uses_instagram == "Yes") +
                     (uses_tiktok == "Yes"),
    
    across(
      starts_with("fb_why_"), 
      ~ if_else(. == "Yes", 1, 0),
      .names = "{.col}_n" 
    )

  ) %>% 
  
  mutate(
    fb_uses_count = rowSums(
      across(starts_with("fb_why_") & ends_with("_n")), 
      na.rm = TRUE
    )
  )

glimpse(w144_wrangled)
```

  * **1. Load the tidyverse:** Loads the `tidyverse` library.
  * **2. Find Path:** `system.file()` is the standard way to find package files *inside* the 'mccoursepack' package.
  * **3. Load Data:** Loads the data from the full path.
  * **4. Glimpse:** Confirms the raw `w144_data` is loaded.
  * **5. Run Wrangling Steps:**
      * (Inside first `mutate`): Create 'party\_simple'
      * (Inside first `mutate`): Create 'platform\_count'
      * (Inside first `mutate`): Recode 'fb\_why' items to 1/0
      * (Inside second `mutate`): Create 'fb\_uses\_count' scale
  * **6. Check Your Work:** Glimpse your final `w144_wrangled` object to make sure it's ready.

After running this, your **Environment** should contain the `w144_wrangled` object. We are now ready to test hypotheses.

-----

## **Part 2: What is Hypothesis Testing? (The 30-Second Version)**

In inferential statistics, we are always testing a **Null Hypothesis (H0)**.

  * **Null Hypothesis (H0):** The "boring" hypothesis. It states there is **no** relationship, **no** association, or **no** difference between groups. (e.g., "There is no difference in platform use between men and women.")
  * **Alternative Hypothesis (HA):** The "interesting" hypothesis. It states that there *is* a relationship, association, or difference. (e.g., "There *is* a difference in platform use between men and women.")

We run a test to get a **p-value**.

  * The **p-value** is the *probability of getting our data (or more extreme) if the Null Hypothesis (H0) were true.*
  * A tiny p-value means: "It's *very unlikely* we'd see this data if H0 were true."
  * **The Rule:** If **`p < .05`** (less than 5% probability), we **reject the Null Hypothesis (H0)**. We conclude that our finding is **statistically significant** and that our Alternative Hypothesis (HA) is likely true.

-----

## **Part 3: Test 1 - The Chi-Square (œá¬≤) Test**

**Use Case:** When you are testing for an **association** between two **categorical** variables.

  * **Research Question:** "Is there a statistically significant *association* between political party (`party_simple`) and TikTok use (`uses_tiktok`)?"
  * **H0 (Null):** There is **no association** between `party_simple` and `uses_tiktok`. (The variables are independent.)
  * **HA (Alternative):** There *is* an association between `party_simple` and `uses_tiktok`.

### **Step 1: Create a Contingency Table (Crosstab)**

It's always good practice to look at the numbers first. The `table()` function is great for this.

```{r}
party_tiktok_table <- table(w144_wrangled$party_simple, 
                            w144_wrangled$uses_tiktok)

print(party_tiktok_table)
```

  * This creates a contingency table (crosstab) of the two categorical variables.
  * `print()` displays the table in the Console.

*You can see the raw counts. But are these counts "different enough" from what we'd expect by chance?*

### **Step 2: Run the Chi-Square Test**

The `chisq.test()` function runs the test directly on your table.

```{r}
chisq.test(party_tiktok_table)
```

  * Runs the Chi-Square test on the `party_tiktok_table` object you just created.

### **Step 3: Interpret the Result**

Look in your Console at the output. You'll see `X-squared = ...`, `df = ...`, and `p-value = ...`.

```
        Pearson's Chi-squared test
data:  party_tiktok_table
X-squared = 824.22, df = 2, p-value < 2.2e-16
```

  * **Interpretation:** Our p-value is `< 2.2e-16`. This is scientific notation for a *very, very* small number (0.000...0022).
  * This p-value is *much less than 0.05*.
  * **Conclusion:** We **reject the Null Hypothesis (H0)**. We conclude that there *is* a statistically significant association between a person's political party and whether or not they use TikTok.

-----

## **Part 4: Test 2 - The Independent Samples T-Test**

**Use Case:** When you are testing for a **difference** in a **numeric** variable between **two categorical** groups.

  * **Research Question:** "Is there a statistically significant *difference* in `platform_count` (numeric) between men and women (`gender`)?"
  * **H0 (Null):** The mean `platform_count` for men is **the same as** the mean `platform_count` for women. (Mean 1 = Mean 2)
  * **HA (Alternative):** The mean `platform_count` for men is **not the same as** the mean `platform_count` for women. (Mean 1 ‚â† Mean 2)

### **Step 1: Run the T-Test**

We use the `t.test()` function. The formula syntax `y ~ x` (read as "y by x") is the easiest way. `platform_count ~ gender`.

**Important:** The `t.test()` function can be fussy about `NA` (missing) values. It's safest to create a small, filtered dataset first.

```{r}
gender_data <- w144_wrangled %>% 
  filter(!is.na(gender))

t.test(platform_count ~ gender, data = gender_data)
```

  * **Step 1:** Create a new dataset (`gender_data`) that removes rows where the `gender` column has a missing (`NA`) value. `is.na()` checks for missing, and `!` means "NOT".
  * **Step 2:** Run the `t.test` on this new, clean `gender_data`, using the formula `platform_count ~ gender` ("platform\_count by gender").

### **Step 2: Interpret the Result**

Look at the output in your Console.

```
        Welch Two Sample t-test
data:  platform_count by gender
t = 2.0594, df = 9976.2, p-value = 0.03947
...
mean in group Female mean in group Male 
           2.036087            1.986872 
```

  * **Interpretation:** Our `p-value` is `0.03947`.
  * This p-value is *less than 0.05*.
  * **Conclusion:** We **reject the Null Hypothesis (H0)**. We conclude there *is* a statistically significant difference in the average number of platforms used by men and women.
  * The output also gives us the means: Females in the sample used an average of 2.04 platforms, while Males used 1.99. The t-test confirms this small difference is *statistically significant*.

-----

## **Part 5: Test 3 - Analysis of Variance (ANOVA)**

**Use Case:** When you are testing for a **difference** in a **numeric** variable between **three or more categorical** groups. (It's like a t-test for 3+ groups).

  * **Research Question:** "Is there a statistically significant *difference* in `platform_count` (numeric) across our four `age_group` categories?"
  * **H0 (Null):** The mean `platform_count` is **the same** for all four age groups. (Mean 1 = Mean 2 = Mean 3 = Mean 4)
  * **HA (Alternative):** **At least one** of the age group means is different from the others.

### **Step 1: Create the ANOVA Model**

We use the `aov()` (Analysis of Variance) function. The syntax is the same formula: `y ~ x`.

```{r}
anova_model <- aov(platform_count ~ age_group, data = w144_wrangled)
```

  * **Step 1:** Create the model object using `aov()` (analysis of variance).
  * The formula `platform_count ~ age_group` tests the `platform_count` variable *by* the `age_group` categories.

### **Step 2: Get the Summary Table**

You must call `summary()` on the model to see the results.

```{r}
summary(anova_model)
```

  * **Step 2:** Look at the summary of the model object to see the results table.

### **Step 3: Interpret the Result**

The output in your Console is a table. Find the row for your variable (`age_group`).

```
                 Df Sum Sq Mean Sq F value Pr(>F)    
age_group         3   2817   939.1   554.2 <2e-16 ***
Residuals     10166  17234     1.7                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
(284 observations deleted due to missingness)
```

  * **Interpretation:** Find the `Pr(>F)` column. This is your p-value.
  * Our p-value is `< 2e-16`, which is *much less than 0.05*.
  * **Conclusion:** We **reject the Null Hypothesis (H0)**. We conclude that there *is* a statistically significant difference in mean `platform_count` somewhere between our four age groups.

-----

## **Part 6: The "So What?" Test (ANOVA Post-Hoc)**

The ANOVA test is "dumb." It tells us "a difference *exists*" but not *which groups* are different. Is "18-29" different from "30-49"? Is "30-49" different from "65+"? We don't know.

We must run a **post-hoc test** (meaning "after this") to find out. We'll use `TukeyHSD()` (Tukey's Honest Significant Difference).

### **Step 1: Run the TukeyHSD() Test**

You just run this function on your `anova_model` object.

```{r}
TukeyHSD(anova_model)
```

  * Runs the `TukeyHSD()` (Tukey's Honest Significant Difference) post-hoc test on the `anova_model` object.

### **Step 2: Interpret the Result**

This output is a big table of *all possible pairs*. The only column you care about is `p adj` (the "adjusted p-value").

```
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = platform_count ~ age_group, data = w144_wrangled)

$age_group
                   diff        lwr        upr   p adj
30-49-18-29   -0.5750036 -0.66981880 -0.4801884 < 2e-16 ***
50-64-18-29   -1.1274577 -1.22389148 -1.0310240 < 2e-16 ***
65+-18-29     -1.6373859 -1.73489115 -1.5398807 < 2e-16 ***
50-64-30-49   -0.5524541 -0.63580447 -0.4691038 < 2e-16 ***
65+-30-49     -1.0623823 -1.14710202 -0.9776626 < 2e-16 ***
65+-50-64     -0.5099282 -0.59620563 -0.4236508 < 2e-16 ***
```

  * **Interpretation:** Look at the `p adj` column for every row.
  * The p-value for `30-49-18-29` is `< 2e-16` (which is \< 0.05). This means the "18-29" and "30-49" groups are significantly different.
  * The p-value for `50-64-18-29` is `< 2e-16`. They are also different.
  * ... in fact, **all pairs** have a `p adj < .05`.
  * **Conclusion:** This test gives us the full story: *Every single age group is statistically significantly different from every other age group* in their average platform use. This confirms what we saw in our Week 13 visualization\!

  -----

  ## **Part 7: Regression Analysis**

  Regression analysis allows us to examine the relationship between one numeric outcome (dependent variable) and one or more predictor (independent) variables. Here, we will cover both simple (linear) and multiple linear regression using the `w144_wrangled` dataset.

  ### **A. Simple Linear Regression**

  **Use Case:** Predict a numeric outcome from a single numeric predictor.

  * **Research Question:** Is there a relationship between age (as a numeric variable) and the number of platforms used (`platform_count`)?
  * **H0 (Null):** There is no linear relationship between age and platform count (slope = 0).
  * **HA (Alternative):** There is a linear relationship (slope ‚â† 0).

  #### **Step 1: Fit the Linear Regression Model**

  ```{r}
  # Make sure age is numeric. If not, convert it.
  w144_wrangled <- w144_wrangled %>% mutate(age_num = as.numeric(age))

  # Remove missing values for age_num and platform_count
  reg_data <- w144_wrangled %>% filter(!is.na(age_num), !is.na(platform_count))

  # Fit the model
  lm_simple <- lm(platform_count ~ age_num, data = reg_data)

  # View the summary
  summary(lm_simple)
  ```

  #### **Step 2: Interpret the Result**

  Look at the `summary(lm_simple)` output. Focus on the `age_num` row:

  * The **Estimate** for `age_num` is the slope. If negative, platform use decreases as age increases.
  * The **p-value** tests if the slope is significantly different from zero.
  * If `p < .05`, we reject H0 and conclude there is a significant linear relationship between age and platform count.

  -----

  ### **B. Multiple Linear Regression**

  **Use Case:** Predict a numeric outcome from two or more predictors (can be numeric or categorical).

  * **Research Question:** How do age, gender, and political party together predict the number of platforms used?
  * **H0 (Null):** None of the predictors are related to platform count (all slopes = 0).
  * **HA (Alternative):** At least one predictor is related to platform count.

  #### **Step 1: Fit the Multiple Regression Model**

  ```{r}
  # Remove missing values for all predictors
  multi_reg_data <- w144_wrangled %>% 
    filter(!is.na(age_num), !is.na(gender), !is.na(party_simple), !is.na(platform_count))

  # Fit the model (gender and party_simple are categorical)
  lm_multi <- lm(platform_count ~ age_num + gender + party_simple, data = multi_reg_data)

  # View the summary
  summary(lm_multi)
  ```

  #### **Step 2: Interpret the Result**

  * Each row in the output shows the effect of a predictor, controlling for the others.
  * The **p-value** for each predictor tests if it is significantly related to platform count.
  * If `p < .05` for a predictor, it is a significant predictor of platform count.
  * The **Intercept** is the expected platform count for the reference group (e.g., baseline gender and party).

  -----

  **Summary:**

  Regression analysis allows you to model and interpret the relationship between a numeric outcome and one or more predictors. Use simple regression for one predictor, and multiple regression for several predictors (including categorical variables).
