% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode,bookmarksopen=true,bookmarksnumbered=true}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  12pt,
  letterpaper,
  oneside]{book}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
  \setmainfont[]{Times New Roman}
  \setsansfont[]{Arial}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{setspace}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\author{}
\date{}
\begin{document}
\frontmatter

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\setstretch{1.5}
\mainmatter
\chapter*{Making Inferences --- Hypothesis Testing and
Reporting}\label{making-inferences-hypothesis-testing-and-reporting}
\addcontentsline{toc}{chapter}{Making Inferences --- Hypothesis Testing
and Reporting}

\section*{The Leap from Sample to
Population}\label{the-leap-from-sample-to-population}
\addcontentsline{toc}{section}{The Leap from Sample to Population}

In the previous chapter, we explored the essential first step of data
analysis: describing our data. Through the tools of descriptive
statistics and data visualization, we learned how to take a raw dataset
and distill it into a coherent and understandable summary. We can now
confidently describe the central tendency, spread, and shape of the
variables within our sample. We can state the mean age of the 500
university students we surveyed, or visualize the distribution of their
social media usage. This is a crucial and illuminating process, but for
much of quantitative research, it is only the beginning of the journey.

The ultimate goal of most social scientific inquiry is not simply to
describe the specific sample we have collected, but to say something
meaningful about the larger, unobserved \textbf{population} from which
that sample was drawn. We want to move from the particular to the
general. We want to take the findings from our 500 students and make a
reasonable claim about the media habits of all 20,000 students at the
university. This is the act of \textbf{statistical inference}: the
process of using data from a sample to draw conclusions or make educated
guesses about a population. It is a logical and mathematical leap of
faith, a journey from the known to the unknown.

How can we make this leap with any degree of confidence? How do we know
if a pattern we observe in our sample---a difference between two groups
or a relationship between two variables---is a ``real'' pattern that
likely exists in the broader population, or if it is merely a fluke, a
random artifact of the specific individuals who happened to end up in
our sample? This is the central question that \textbf{hypothesis
testing} is designed to answer. It is a systematic framework for making
decisions under conditions of uncertainty. It is the formal process by
which we use the laws of probability to evaluate the evidence from our
sample and make a disciplined judgment about our research hypotheses.

This chapter is the culmination of our journey through the quantitative
research workflow. It demystifies the logic of inferential statistics,
focusing on the conceptual framework of hypothesis testing rather than
on complex mathematical formulas. We will explore the core concepts that
drive this process, including the crucial role of the null hypothesis,
the meaning of statistical significance and the p-value, and the two
types of errors we risk making in any inferential decision. Critically,
we will distinguish between a finding that is statistically significant
and one that is practically meaningful by introducing the essential
concept of \textbf{effect size}. Finally, we will provide a conceptual
guide to choosing the correct statistical test for your research
question and offer a clear blueprint for how to report your findings
transparently and responsibly.

\section*{The Logic of Hypothesis Testing: A Framework for
Decision-Making}\label{the-logic-of-hypothesis-testing-a-framework-for-decision-making}
\addcontentsline{toc}{section}{The Logic of Hypothesis Testing: A
Framework for Decision-Making}

At its heart, hypothesis testing is a formal procedure for making a
decision about a knowledge claim. It is a structured argument that pits
two competing statements against each other: the null hypothesis and the
research hypothesis.

As we discussed in Chapter 6, the \textbf{null hypothesis (H0)} is the
hypothesis of ``no difference'' or ``no relationship.'' It is a
statement of equality, proposing that in the population, the independent
variable has no effect on the dependent variable. The \textbf{research
hypothesis (H1 or HA)}, by contrast, is a statement of inequality,
proposing that a relationship or difference does exist. The entire
logical apparatus of hypothesis testing is built around a conservative
and skeptical approach: we never set out to ``prove'' our research
hypothesis. Instead, we start by assuming the null hypothesis is true
and then evaluate whether the evidence from our sample is strong enough
to make that assumption untenable. Our goal is to gather enough evidence
to confidently \textbf{reject the null hypothesis}.

This process is designed to answer a single, fundamental question: ``Is
the pattern I observed in my sample data so strong and clear that it is
unlikely to have occurred simply due to random chance?''

Imagine you conduct an experiment to test whether a new media literacy
curriculum (the independent variable) improves students' ability to
identify misinformation (the dependent variable). You find that the
students in your treatment group, who received the curriculum, scored an
average of 10 points higher on a misinformation test than the students
in the control group. This 10-point difference is the observed effect in
your sample. But could this difference have happened just by luck? Is it
possible that, by pure chance, you happened to randomly assign the
slightly more savvy students to the treatment group? Hypothesis testing
is the tool that allows us to calculate the probability of getting a
10-point difference (or an even larger one) if the curriculum actually
had no effect at all (i.e., if the null hypothesis were true). If that
probability is very low, we can reject the ``it was just luck''
explanation and conclude that the curriculum likely had a real effect.

\section*{The Key Concepts of Significance
Testing}\label{the-key-concepts-of-significance-testing}
\addcontentsline{toc}{section}{The Key Concepts of Significance Testing}

This process of evaluating probabilities is formalized through a set of
key concepts that form the language of inferential statistics.
Understanding these concepts is essential for both conducting and
consuming quantitative research.

\subsection*{The p-value and Statistical
Significance}\label{the-p-value-and-statistical-significance}
\addcontentsline{toc}{subsection}{The p-value and Statistical
Significance}

The central output of any statistical test is the \textbf{p-value}. The
\textbf{p-value} is the probability of observing your sample result (or
a more extreme result) if the null hypothesis were actually true in the
population. It is a measure of how surprising or unlikely your data is,
assuming there is no real effect.

\begin{itemize}
\item
  A \textbf{large p-value} (e.g., p =.40) means that your observed
  result is not very surprising. There is a 40\% chance of getting a
  result like yours even if the null hypothesis is true. This is not
  strong evidence against the null hypothesis.
\item
  A \textbf{small p-value} (e.g., p =.01) means that your observed
  result is very surprising. There is only a 1\% chance of getting a
  result this extreme if the null hypothesis is true. This provides
  strong evidence against the null hypothesis.
\end{itemize}

But how small is ``small enough''? Before conducting the analysis,
researchers set a threshold for this probability, a criterion for how
much evidence they will require before they are willing to reject the
null hypothesis. This threshold is called the \textbf{alpha level (α)},
or the \textbf{significance level}. The conventional standard in most
social sciences, including communication, is to set the alpha level at
\textbf{.05}.

This leads to a simple decision rule:

\begin{itemize}
\item
  If the \textbf{p-value is less than or equal to the alpha level (p
  ≤.05)}, we \textbf{reject the null hypothesis}. We conclude that our
  finding is \textbf{statistically significant}, meaning it is unlikely
  to be the result of random chance.
\item
  If the \textbf{p-value is greater than the alpha level (p
  \textgreater.05)}, we \textbf{fail to reject the null hypothesis}. We
  conclude that our finding is not statistically significant, meaning we
  do not have sufficient evidence to rule out the possibility that our
  result is due to chance.
\end{itemize}

It is crucial to use this precise and cautious language. We never
``prove'' the research hypothesis, because there is always a small
probability that we are wrong. And we never ``accept'' the null
hypothesis, because a lack of evidence for an effect is not the same as
evidence for a lack of an effect.

\subsection*{Type I and Type II Errors: The Risks of
Decision-Making}\label{type-i-and-type-ii-errors-the-risks-of-decision-making}
\addcontentsline{toc}{subsection}{Type I and Type II Errors: The Risks
of Decision-Making}

Because we are making decisions based on the incomplete information from
a sample, we always run the risk of making an error. In hypothesis
testing, there are two specific types of errors we can make.

\begin{itemize}
\item
  \textbf{Type I Error (a ``False Positive''):} This occurs when we
  \textbf{reject a true null hypothesis}. In other words, we conclude
  that there is an effect or a relationship in the population when, in
  reality, there is not one. Our sample data misled us, likely due to
  random chance. The probability of making a Type I error is directly
  controlled by the alpha level we set. If we set α =.05, we are
  accepting a 5\% risk of making a Type I error.
\item
  \textbf{Type II Error (a ``False Negative''):} This occurs when we
  \textbf{fail to reject a false null hypothesis}. In this case, there
  really is an effect or relationship in the population, but our study
  failed to detect it. This often happens when a study has too small a
  sample size to detect a real but subtle effect.
\end{itemize}

There is an inherent trade-off between these two types of errors. If we
make it harder to commit a Type I error (e.g., by setting a more
stringent alpha level, like α =.01), we simultaneously increase the
probability of committing a Type II error. The conventional α =.05 is
seen as a reasonable balance between these two risks for most social
science research.

\subsection*{Statistical Power}\label{statistical-power}
\addcontentsline{toc}{subsection}{Statistical Power}

Related to Type II error is the concept of \textbf{statistical power}.
Power is the probability of correctly rejecting a false null hypothesis.
In simpler terms, it is the probability that your study will detect an
effect that actually exists. The conventional standard is to aim for a
power of.80, which means accepting a 20\% chance of committing a Type II
error. Power is influenced by three main factors: the alpha level, the
sample size, and the size of the effect in the population. The most
direct way for a researcher to increase the power of their study is to
increase their sample size.

\section*{Significance vs.~Meaningfulness: The Importance of Effect
Size}\label{significance-vs.-meaningfulness-the-importance-of-effect-size}
\addcontentsline{toc}{section}{Significance vs.~Meaningfulness: The
Importance of Effect Size}

One of the most common and critical errors in interpreting quantitative
research is to equate statistical significance with practical
importance. A statistically significant result simply tells us that an
observed effect is unlikely to be zero in the population. It does not,
by itself, tell us how

large, strong, or meaningful that effect is.

This distinction is crucial because statistical significance is heavily
influenced by sample size. With a very large sample, even a tiny,
trivial, and practically meaningless effect can become statistically
significant. For example, with a sample of 300,000 people, we might find
a statistically significant difference in IQ between two groups, but
that difference might be only a fraction of a single IQ point---a
difference that has no real-world importance.

To address this, responsible researchers report not only the statistical
significance of their findings but also the \textbf{effect size}. An
\textbf{effect size} is a standardized statistic that measures the
magnitude or strength of the effect or relationship, independent of the
sample size. It answers the ``so what?'' question: How big is the
difference? How strong is the relationship?

Reporting both the p-value and the effect size provides a complete
picture.

\begin{itemize}
\item
  The \textbf{p-value} tells us about our confidence that an effect is
  ``real'' (i.e., not due to chance).
\item
  The \textbf{effect size} tells us about the practical importance or
  magnitude of that effect.
\end{itemize}

A finding with a small p-value and a large effect size is the most
compelling result. A finding with a small p-value but a tiny effect size
may be statistically real but practically irrelevant. A finding with a
large effect size but a large p-value might suggest a meaningful effect
that the study was simply underpowered (due to a small sample) to detect
with statistical confidence.

\section*{A Conceptual Guide to Common Inferential Statistical
Tests}\label{a-conceptual-guide-to-common-inferential-statistical-tests}
\addcontentsline{toc}{section}{A Conceptual Guide to Common Inferential
Statistical Tests}

The specific statistical test a researcher uses to calculate a p-value
depends on their research question, the level of measurement of their
variables, and their research design. While the mathematical formulas
differ, the underlying logic of hypothesis testing is the same for all
of them. Here is a conceptual guide to some of the most common tests.

\subsection*{Tests of Difference (Comparing Group
Means)}\label{tests-of-difference-comparing-group-means}
\addcontentsline{toc}{subsection}{Tests of Difference (Comparing Group
Means)}

\textbf{t-test:} This test is used to compare the means of \textbf{two}
groups.

\begin{itemize}
\tightlist
\item
  An \textbf{independent samples t-test} is used when the two groups are
  independent of each other (e.g., an experimental group vs.~a control
  group).
\item
  A \textbf{paired samples t-test} is used when the two sets of scores
  come from the same participants measured at two different times (e.g.,
  a pretest and a posttest).
\end{itemize}

\textbf{Analysis of Variance (ANOVA):} This test is used to compare the
means of \textbf{three or more} groups. An ANOVA will tell you if there
is a significant difference somewhere among the group means, but it will
not tell you which specific groups differ from each other. To find that
out, a researcher must follow up a significant ANOVA result with
\textbf{post hoc tests} (like the Tukey HSD test), which conduct
pairwise comparisons between all the groups.

\subsection*{Tests of Association (Examining
Relationships)}\label{tests-of-association-examining-relationships}
\addcontentsline{toc}{subsection}{Tests of Association (Examining
Relationships)}

\begin{itemize}
\item
  \textbf{Chi-Square Test:} This test is used to examine the
  relationship between two \textbf{categorical (nominal)} variables. It
  compares the observed frequencies in a contingency table to the
  frequencies that would be expected if there were no relationship
  between the variables.
\item
  \textbf{Correlation:} This test measures the strength and direction of
  the linear relationship between two \textbf{continuous
  (interval/ratio)} variables. The result is a correlation coefficient
  (r) that ranges from -1.0 to +1.0.
\item
  \textbf{Regression:} This is a more advanced technique used to
  \textbf{predict} the value of one continuous dependent variable from
  one or more independent variables. It allows researchers to assess the
  unique contribution of each predictor variable while controlling for
  the effects of the others.
\end{itemize}

\section*{Reporting the Results: Transparency and
Precision}\label{reporting-the-results-transparency-and-precision}
\addcontentsline{toc}{section}{Reporting the Results: Transparency and
Precision}

The final stage of the research process is to communicate your findings
to others. The \textbf{Results} section of a formal research paper is a
direct, objective, and journalistic account of the outcomes of your data
analysis. It should be organized around your research questions and
hypotheses, presenting the evidence in a clear and logical sequence.

For each hypothesis or research question, a well-written results section
should do the following:1

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Restate the hypothesis or research question} being tested.
\item
  \textbf{Identify the statistical test} used to evaluate it.
\item
  \textbf{Report the key descriptive statistics} that are relevant to
  the test (e.g., the means and standard deviations for the groups being
  compared in a t-test).
\item
  \textbf{Report the results of the inferential test} in the standard
  format required by the relevant style guide (such as APA). This
  typically includes the test statistic (e.g., t, F, r, χ²), the degrees
  of freedom, the obtained value of the statistic, the p-value, and the
  effect size.
\item
  \textbf{State in plain English whether the hypothesis was supported or
  not} (i.e., whether the null hypothesis was rejected). Avoid the word
  ``prove.'' Instead, use cautious language like ``the hypothesis was
  supported'' or ``the results are consistent with the hypothesis.''
\end{enumerate}

It is crucial to distinguish the Results section from the
\textbf{Discussion} section. The Results section simply reports the
findings without interpretation. The Discussion section is where you
interpret those findings, explaining what they mean, how they relate to
the literature and theory you presented in your introduction,
acknowledging the study's limitations, and suggesting directions for
future research.

\section*{Conclusion: The Responsible Interpretation of
Evidence}\label{conclusion-the-responsible-interpretation-of-evidence}
\addcontentsline{toc}{section}{Conclusion: The Responsible
Interpretation of Evidence}

The journey from a sample to a population is the central challenge of
quantitative research. Statistical inference, through the framework of
hypothesis testing, provides us with a powerful and disciplined set of
tools for navigating this journey. It allows us to manage uncertainty,
to quantify the strength of our evidence, and to make reasonable
decisions about our knowledge claims based on the laws of probability.

However, these tools must be used with wisdom and humility. We must
remember that statistical significance is not the same as real-world
importance and that our conclusions are always probabilistic, never
absolute. The skills you have learned in this chapter---understanding
the logic of the p-value, appreciating the importance of effect sizes,
and knowing how to interpret and report statistical findings with
precision---are essential for both the responsible production of new
knowledge and the critical consumption of the endless stream of
data-driven claims that define our modern world. They are the tools that
allow us to move from simply describing what we see to making a credible
and evidence-based case for what we believe to be true.

\section*{Journal Prompts}\label{journal-prompts}
\addcontentsline{toc}{section}{Journal Prompts}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  This chapter describes inference as a ``leap'' from sample to
  population. Reflect on what makes that leap trustworthy---or risky.
  Why is it not enough to observe a pattern in your sample? How does
  hypothesis testing help, and what limits remain even when your results
  are statistically significant?
\item
  Many people misunderstand the p-value as ``proof.'' Why is this
  incorrect? What does a small p-value tell us---and what does it
  \emph{not} tell us? Reflect on a time you saw a research claim or news
  headline that leaned too heavily on the idea of ``significance.'' What
  might have been missing?
\item
  Imagine you find a statistically significant result in your
  research---but the effect size is tiny. Would you still report it? Why
  or why not? How do you balance statistical significance with practical
  or social importance? What responsibility do researchers have when
  communicating findings that might be misinterpreted?
\end{enumerate}


\backmatter


\end{document}
